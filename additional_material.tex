%%% Penalty methods

\begin{frame}
\frametitle{Penalty methods and augmented Lagrangian}

Let's turn to {\red nonlinear programming}.

\mbox{}

Consider the optimization problem with the equality constraints
\begin{align*}
\min_x\ & f(x) \\
\mbox{s.t. } & c_i(x) = 0,\quad i \in \mathcal{E}.
\end{align*}

The idea consists to include the equality constraints in a penalty term that measures the constraints violation and that is added to the objective.

\mbox{}

The penalization term is non-null when $x$ is not feasible with respect to the constraints.
For this reason, such approaches are also known as {\blue exterior penalization methods}.

\end{frame}

\begin{frame}
\frametitle{Penalization methods}

The quadratic penalty method uses
\[
P(x;\mu) \overset{def}{=} \frac{1}{2\mu} \sum_{i \in \mathcal{E}} c_i^2(x)
\]
as a penalization term, where the penalty parameter is given by $\rho = \frac{1}{2\mu} > 0$.
At iteration $k$, we minimize the function
\[
f(x_k) + P(x_k;\mu_k),
\]
where $\mu_k$ is chosen such that $\mu_k \rightarrow 0$ when $k \rightarrow \infty$.

\mbox{}

When inequality constraints are present (with the form $c_i(x) \geq
0$), the quadratic penalty function is augmented by a term reflecting their violation, and becomes
\[
P(x;\mu) \overset{def}{=} \frac{1}{2\mu} \sum_{i \in \mathcal{E}}
c_i^2(x) + \frac{1}{2\mu} \sum_{i \in \mathcal{I}} (\max \lbrace
-c_i(x), 0 \rbrace)^2.
\]

\end{frame}

\begin{frame}
\frametitle{Penalization methods (cont'd)}

Adding such terms can however lead to a less smooth penalty function, which of course less appealing in a minization context.
More generally, a major drawback of penalization methods is that the Hessian $\nabla_{xx}^2 P(x; \mu)$ becomes usually ill-conditioned close to the minimizer of  $P(x; \mu)$ when $\mu \rightarrow 0$.

\mbox{}

Another type of penalization term is given by
\[
P(x;\mu) \overset{def}{=} \frac{1}{\mu} \sum_{i \in \mathcal{I}}
\max \lbrace 0, -c_i(x) \rbrace + \frac{1}{\mu} \sum_{i \in
\mathcal{E}} |c_i(x)|.
\]

\end{frame}

\begin{frame}
\frametitle{Penalization methods (cont'd)}

It can be shown that $\phi(x; \mu) \overset{def}{=} f(x) + P(x;\mu)$, called {\blue exact merit function $\ell_1$}, satisfies the following definition.
\begin{definition}[Exact merit function]
A merit function $\psi(x; \mu)$ is said exact if it exists a positive scalar  $\mu^*$ such that for any $\mu \in (0, \mu^*]$, any local solution of the nonlinear programming problem is a local minimizer of $\psi(x;\mu)$.
\end{definition}

\begin{itemize}
\item
It is however difficut in practice to choose the adequate value of  $\mu$ before an iteration when solving most of the problems.
\item
Exact penalty functions are often non-differentiable.
Example: the first derivate of the function $\ell_1$ is not defined for any $x$ such that $c_i(x) =
0$ ($i \in \mathcal{E} \cup \mathcal{I}$).
\end{itemize}

\end{frame}
